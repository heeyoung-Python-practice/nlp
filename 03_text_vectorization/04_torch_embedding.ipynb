{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a4465e",
   "metadata": {},
   "source": [
    "# torch `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb451a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80ee6a",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95446fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [          \n",
    "    'nice great best amazing',  # 긍정 문장 예시\n",
    "    'stop lies',                # 부정/비판 문장 예시\n",
    "    'pitiful nerd',             # 부정 문장 예시\n",
    "    'excellent work',           # 긍정 문장 예시\n",
    "    'supreme quality',          # 긍정 문장 예시\n",
    "    'bad',                      # 부정 문장 예시\n",
    "    'highly respectable'        # 긍정 문장 예시\n",
    "]                               # 분류 모델에 넣을 입력 문장 리스트(list[str])\n",
    "labels = [1, 0, 0, 1, 1, 0, 1]  # 각 문장에 대한 이진 라벨(1=긍정, 0=부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be17ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nice', 'great', 'best', 'amazing'],\n",
       " ['stop', 'lies'],\n",
       " ['pitiful', 'nerd'],\n",
       " ['excellent', 'work'],\n",
       " ['supreme', 'quality'],\n",
       " ['bad'],\n",
       " ['highly', 'respectable']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences]    # 긱 믄장을 토큰리스트(list(list[str]))로 변환\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd2df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nice': 1, 'great': 1, 'best': 1, 'amazing': 1, 'stop': 1, 'lies': 1, 'pitiful': 1, 'nerd': 1, 'excellent': 1, 'work': 1, 'supreme': 1, 'quality': 1, 'bad': 1, 'highly': 1, 'respectable': 1})\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성 + 정수 인쾽\n",
    "from collections import Counter\n",
    "\n",
    "tokens = [token for sent in tokenized_sentences for token in sent]   # 문장리스트를 평탄화하여 전체 토큰 리스트 생성\n",
    "word_counts = Counter(tokens)\n",
    "print(word_counts)\n",
    "\n",
    "word_to_index = {word:index +2 for index,word in enumerate(tokens)}\n",
    "word_to_index['<PAD>']=0    # 패딩토큰(길이맞추기)\n",
    "word_to_index['<UNK>']=1    # OOV토큰(처리불가 단어 대체)\n",
    "word_to_index = dict(sorted(word_to_index.items(),key=lambda x:x[1]))   # 인덱스를 기준으로 정렬\n",
    "print(word_to_index)    # 단어 -> 인덱스 사전\n",
    "\n",
    "vocab_size = len(word_to_index) # 전체 어휘 수 (특수토큰 포함)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d91574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice great best amazing',\n",
       " 'stop lies',\n",
       " 'pitiful nerd',\n",
       " 'excellent work',\n",
       " 'supreme quality',\n",
       " 'bad',\n",
       " 'highly respectable']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩 함수 : 토큰화된 문장 리스트를 단어 -> 인덱스 사전으로 정수 시퀀스 (list[list(int)])로 변환\n",
    "def texts_to_sequence(sentences, word_to_index):\n",
    "    sequences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sequence = []\n",
    "        \n",
    "        for token in sent:\n",
    "            if token in word_to_index:\n",
    "                sequence.append(word_to_index[token])\n",
    "            else :\n",
    "                sequence.append(word_to_index['<UNK>'])\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "sequences = texts_to_sequence(tokenized_sentences, word_to_index)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918eb640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 서로 다른 길이의 정수 시퀀스를 0(<PAD>)으로 채우거나 잘라내  (문장수, maxlen) 형태로 맞춰주는 함수\n",
    "def pad_sequnces(sentences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sentences),maxlen),dtype=int)  # (문장수 x maxlen)크기의 0 패딩 배열\n",
    "    \n",
    "    for index, seq in enumerate(sequences): # 각 문장 시퀀스 순회\n",
    "        padded_sequences[index, :len(seq)] = seq[:maxlen]   # 앞에서부터 시퀀스 채운다. 길면 maxlen까지만 채워 자른다.\n",
    "        \n",
    "    return padded_sequences\n",
    "\n",
    "padded_sequnces = pad_sequnces(sequences,maxlen=4)  # 모든 문장 길이 4로 패딩/자르기\n",
    "padded_sequnces # (문장 수,5) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016934a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequnces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88cd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (embedding): Embedding(17, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 16, batch_first=True)\n",
       "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)ㄹㄹ 출력\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,    # 임베딩 차원\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_size,batch_first=True)   # 입력(배치,길이.차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size,1)     # 맞막 은닉 상태를 1차원 logit으로 변환\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)    # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)   # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))  # (batch_size, hidden_size) -> (batch,1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = 100\n",
    "model = SimpleNet(vocab_size,embedding_dim,hidden_size=16)  # 어휘크기/임베딩차원/은닉크기로 모델 생성\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ed91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b3fac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleNet                                --\n",
       "├─Embedding: 1-1                         1,700\n",
       "├─RNN: 1-2                               1,888\n",
       "├─Linear: 1-3                            17\n",
       "=================================================================\n",
       "Total params: 3,605\n",
       "Trainable params: 3,605\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary   # 모델 구조를 표 형태로 요약\n",
    "\n",
    "summary(model)  # model의 레이어 구성 / 파라미터 수를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4089df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 100])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>-1.735707</td>\n",
       "      <td>-0.338836</td>\n",
       "      <td>0.387560</td>\n",
       "      <td>1.822761</td>\n",
       "      <td>1.180889</td>\n",
       "      <td>0.029844</td>\n",
       "      <td>-1.454998</td>\n",
       "      <td>0.928572</td>\n",
       "      <td>-0.157014</td>\n",
       "      <td>-0.841594</td>\n",
       "      <td>...</td>\n",
       "      <td>2.005783</td>\n",
       "      <td>0.139943</td>\n",
       "      <td>-0.994724</td>\n",
       "      <td>-1.118804</td>\n",
       "      <td>1.153105</td>\n",
       "      <td>-0.068381</td>\n",
       "      <td>0.769769</td>\n",
       "      <td>-0.516143</td>\n",
       "      <td>1.574835</td>\n",
       "      <td>-0.292705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.475319</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>0.765528</td>\n",
       "      <td>-0.052871</td>\n",
       "      <td>-0.705285</td>\n",
       "      <td>-0.378955</td>\n",
       "      <td>-0.748233</td>\n",
       "      <td>0.525564</td>\n",
       "      <td>0.641379</td>\n",
       "      <td>-0.454036</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.955102</td>\n",
       "      <td>-0.937681</td>\n",
       "      <td>-0.169070</td>\n",
       "      <td>0.885088</td>\n",
       "      <td>-1.246579</td>\n",
       "      <td>0.048523</td>\n",
       "      <td>-0.445946</td>\n",
       "      <td>0.404705</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>-1.336465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.499201</td>\n",
       "      <td>-0.715669</td>\n",
       "      <td>0.121338</td>\n",
       "      <td>-0.448419</td>\n",
       "      <td>-0.066340</td>\n",
       "      <td>1.636819</td>\n",
       "      <td>-0.290606</td>\n",
       "      <td>-0.834929</td>\n",
       "      <td>0.167324</td>\n",
       "      <td>-0.273061</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.352499</td>\n",
       "      <td>1.635663</td>\n",
       "      <td>0.460028</td>\n",
       "      <td>-0.822666</td>\n",
       "      <td>-0.853711</td>\n",
       "      <td>-0.232818</td>\n",
       "      <td>1.406272</td>\n",
       "      <td>-0.409277</td>\n",
       "      <td>-3.135439</td>\n",
       "      <td>0.809925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.019643</td>\n",
       "      <td>1.520321</td>\n",
       "      <td>1.153095</td>\n",
       "      <td>-2.111665</td>\n",
       "      <td>-0.581775</td>\n",
       "      <td>0.603003</td>\n",
       "      <td>0.829944</td>\n",
       "      <td>-0.297855</td>\n",
       "      <td>1.255378</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.804685</td>\n",
       "      <td>0.860246</td>\n",
       "      <td>-2.392415</td>\n",
       "      <td>0.597881</td>\n",
       "      <td>1.046835</td>\n",
       "      <td>-0.689517</td>\n",
       "      <td>0.501067</td>\n",
       "      <td>-0.220726</td>\n",
       "      <td>1.172062</td>\n",
       "      <td>-0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>-0.528966</td>\n",
       "      <td>0.430008</td>\n",
       "      <td>0.344425</td>\n",
       "      <td>-1.987139</td>\n",
       "      <td>-1.231478</td>\n",
       "      <td>-0.105404</td>\n",
       "      <td>0.144557</td>\n",
       "      <td>-0.806390</td>\n",
       "      <td>0.652106</td>\n",
       "      <td>1.586295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255125</td>\n",
       "      <td>0.879363</td>\n",
       "      <td>-0.357278</td>\n",
       "      <td>-0.204549</td>\n",
       "      <td>0.989971</td>\n",
       "      <td>-1.906618</td>\n",
       "      <td>-0.715281</td>\n",
       "      <td>-0.508421</td>\n",
       "      <td>0.664267</td>\n",
       "      <td>1.338793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>0.313743</td>\n",
       "      <td>1.578746</td>\n",
       "      <td>1.369931</td>\n",
       "      <td>-2.099982</td>\n",
       "      <td>0.089595</td>\n",
       "      <td>0.402788</td>\n",
       "      <td>0.245792</td>\n",
       "      <td>-1.682907</td>\n",
       "      <td>-0.575671</td>\n",
       "      <td>0.194547</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.122362</td>\n",
       "      <td>-1.825266</td>\n",
       "      <td>0.916183</td>\n",
       "      <td>-0.624366</td>\n",
       "      <td>1.730734</td>\n",
       "      <td>1.245013</td>\n",
       "      <td>-0.740861</td>\n",
       "      <td>-0.352895</td>\n",
       "      <td>0.413123</td>\n",
       "      <td>0.835810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>-0.717076</td>\n",
       "      <td>-0.107237</td>\n",
       "      <td>1.909080</td>\n",
       "      <td>-0.956966</td>\n",
       "      <td>0.256004</td>\n",
       "      <td>-0.802785</td>\n",
       "      <td>-0.102114</td>\n",
       "      <td>1.818220</td>\n",
       "      <td>-1.332305</td>\n",
       "      <td>-0.597572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479183</td>\n",
       "      <td>-0.414356</td>\n",
       "      <td>0.223169</td>\n",
       "      <td>-1.400546</td>\n",
       "      <td>1.207151</td>\n",
       "      <td>0.776599</td>\n",
       "      <td>-0.996237</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>0.116161</td>\n",
       "      <td>0.337716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.832960</td>\n",
       "      <td>1.364188</td>\n",
       "      <td>1.540450</td>\n",
       "      <td>1.477358</td>\n",
       "      <td>-1.342322</td>\n",
       "      <td>0.310260</td>\n",
       "      <td>-0.095891</td>\n",
       "      <td>-0.227439</td>\n",
       "      <td>-0.187051</td>\n",
       "      <td>-1.081465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064858</td>\n",
       "      <td>1.773807</td>\n",
       "      <td>-1.998466</td>\n",
       "      <td>0.394613</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>1.054239</td>\n",
       "      <td>-0.806519</td>\n",
       "      <td>1.227212</td>\n",
       "      <td>-1.568208</td>\n",
       "      <td>3.540898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>1.308359</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>1.379176</td>\n",
       "      <td>-0.112046</td>\n",
       "      <td>-0.794457</td>\n",
       "      <td>0.633167</td>\n",
       "      <td>0.120520</td>\n",
       "      <td>-0.288433</td>\n",
       "      <td>1.718499</td>\n",
       "      <td>-0.960887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314480</td>\n",
       "      <td>-0.817938</td>\n",
       "      <td>1.107119</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>0.505777</td>\n",
       "      <td>-1.218458</td>\n",
       "      <td>1.140192</td>\n",
       "      <td>-0.387397</td>\n",
       "      <td>-0.013644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>0.588397</td>\n",
       "      <td>-2.922815</td>\n",
       "      <td>1.804515</td>\n",
       "      <td>1.258115</td>\n",
       "      <td>0.676509</td>\n",
       "      <td>-2.781946</td>\n",
       "      <td>-0.884837</td>\n",
       "      <td>-0.551854</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.136410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568573</td>\n",
       "      <td>0.601096</td>\n",
       "      <td>-0.379181</td>\n",
       "      <td>0.106090</td>\n",
       "      <td>-0.318359</td>\n",
       "      <td>-2.015614</td>\n",
       "      <td>-1.582134</td>\n",
       "      <td>3.208747</td>\n",
       "      <td>-0.716470</td>\n",
       "      <td>1.740199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.378477</td>\n",
       "      <td>-0.429661</td>\n",
       "      <td>-1.144708</td>\n",
       "      <td>0.733501</td>\n",
       "      <td>-0.777020</td>\n",
       "      <td>-0.464646</td>\n",
       "      <td>-0.243579</td>\n",
       "      <td>-1.743586</td>\n",
       "      <td>-1.012346</td>\n",
       "      <td>0.902370</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.034917</td>\n",
       "      <td>1.247433</td>\n",
       "      <td>3.093121</td>\n",
       "      <td>-0.385246</td>\n",
       "      <td>0.075501</td>\n",
       "      <td>-0.557319</td>\n",
       "      <td>0.571115</td>\n",
       "      <td>-0.074529</td>\n",
       "      <td>-0.230721</td>\n",
       "      <td>1.586959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>-1.590439</td>\n",
       "      <td>0.616919</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>-0.169033</td>\n",
       "      <td>1.554567</td>\n",
       "      <td>1.993246</td>\n",
       "      <td>-1.499792</td>\n",
       "      <td>0.452104</td>\n",
       "      <td>-0.106776</td>\n",
       "      <td>0.279510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007572</td>\n",
       "      <td>-0.781752</td>\n",
       "      <td>0.889872</td>\n",
       "      <td>-1.179870</td>\n",
       "      <td>-0.792910</td>\n",
       "      <td>-1.446900</td>\n",
       "      <td>0.411810</td>\n",
       "      <td>-0.296746</td>\n",
       "      <td>-1.426191</td>\n",
       "      <td>1.423158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>0.446604</td>\n",
       "      <td>0.469551</td>\n",
       "      <td>-0.434539</td>\n",
       "      <td>0.866039</td>\n",
       "      <td>0.040473</td>\n",
       "      <td>0.294191</td>\n",
       "      <td>-0.140252</td>\n",
       "      <td>-0.226453</td>\n",
       "      <td>-0.284805</td>\n",
       "      <td>0.890681</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266460</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>0.134996</td>\n",
       "      <td>-1.359200</td>\n",
       "      <td>-0.303195</td>\n",
       "      <td>0.526164</td>\n",
       "      <td>-0.502707</td>\n",
       "      <td>0.582444</td>\n",
       "      <td>0.485411</td>\n",
       "      <td>-1.450893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-0.896039</td>\n",
       "      <td>-0.315201</td>\n",
       "      <td>0.541012</td>\n",
       "      <td>-0.946095</td>\n",
       "      <td>-0.629847</td>\n",
       "      <td>1.165045</td>\n",
       "      <td>0.628805</td>\n",
       "      <td>2.609555</td>\n",
       "      <td>0.374878</td>\n",
       "      <td>0.466748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659550</td>\n",
       "      <td>-0.984129</td>\n",
       "      <td>-1.071270</td>\n",
       "      <td>-0.897544</td>\n",
       "      <td>-0.001685</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>-1.872608</td>\n",
       "      <td>-2.139664</td>\n",
       "      <td>0.093976</td>\n",
       "      <td>-1.391590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>-0.373275</td>\n",
       "      <td>-0.400785</td>\n",
       "      <td>-0.339607</td>\n",
       "      <td>0.554397</td>\n",
       "      <td>-0.092210</td>\n",
       "      <td>-2.278988</td>\n",
       "      <td>1.282865</td>\n",
       "      <td>-1.971238</td>\n",
       "      <td>0.286252</td>\n",
       "      <td>-0.012342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186937</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>0.112873</td>\n",
       "      <td>0.387419</td>\n",
       "      <td>1.348049</td>\n",
       "      <td>1.519517</td>\n",
       "      <td>0.293803</td>\n",
       "      <td>-1.134734</td>\n",
       "      <td>-0.881298</td>\n",
       "      <td>0.300755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>-0.189812</td>\n",
       "      <td>-1.106808</td>\n",
       "      <td>-0.562519</td>\n",
       "      <td>-1.127443</td>\n",
       "      <td>1.036392</td>\n",
       "      <td>-0.682786</td>\n",
       "      <td>0.576895</td>\n",
       "      <td>-1.175409</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>-0.620383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703573</td>\n",
       "      <td>-1.360878</td>\n",
       "      <td>0.361350</td>\n",
       "      <td>-0.235250</td>\n",
       "      <td>-1.195826</td>\n",
       "      <td>-0.241891</td>\n",
       "      <td>-1.333441</td>\n",
       "      <td>-0.243081</td>\n",
       "      <td>0.609667</td>\n",
       "      <td>-0.180270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>       -1.735707 -0.338836  0.387560  1.822761  1.180889  0.029844   \n",
       "nice         0.475319 -0.317899  0.765528 -0.052871 -0.705285 -0.378955   \n",
       "great        0.499201 -0.715669  0.121338 -0.448419 -0.066340  1.636819   \n",
       "best         0.019643  1.520321  1.153095 -2.111665 -0.581775  0.603003   \n",
       "amazing     -0.528966  0.430008  0.344425 -1.987139 -1.231478 -0.105404   \n",
       "stop         0.313743  1.578746  1.369931 -2.099982  0.089595  0.402788   \n",
       "lies        -0.717076 -0.107237  1.909080 -0.956966  0.256004 -0.802785   \n",
       "pitiful      0.832960  1.364188  1.540450  1.477358 -1.342322  0.310260   \n",
       "nerd         1.308359  0.048889  1.379176 -0.112046 -0.794457  0.633167   \n",
       "excellent    0.588397 -2.922815  1.804515  1.258115  0.676509 -2.781946   \n",
       "work        -0.378477 -0.429661 -1.144708  0.733501 -0.777020 -0.464646   \n",
       "supreme     -1.590439  0.616919  0.003798 -0.169033  1.554567  1.993246   \n",
       "quality      0.446604  0.469551 -0.434539  0.866039  0.040473  0.294191   \n",
       "bad         -0.896039 -0.315201  0.541012 -0.946095 -0.629847  1.165045   \n",
       "highly      -0.373275 -0.400785 -0.339607  0.554397 -0.092210 -2.278988   \n",
       "respectable -0.189812 -1.106808 -0.562519 -1.127443  1.036392 -0.682786   \n",
       "\n",
       "                   6         7         8         9   ...        90        91  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>       -1.454998  0.928572 -0.157014 -0.841594  ...  2.005783  0.139943   \n",
       "nice        -0.748233  0.525564  0.641379 -0.454036  ... -1.955102 -0.937681   \n",
       "great       -0.290606 -0.834929  0.167324 -0.273061  ... -1.352499  1.635663   \n",
       "best         0.829944 -0.297855  1.255378  0.046525  ... -0.804685  0.860246   \n",
       "amazing      0.144557 -0.806390  0.652106  1.586295  ... -0.255125  0.879363   \n",
       "stop         0.245792 -1.682907 -0.575671  0.194547  ... -1.122362 -1.825266   \n",
       "lies        -0.102114  1.818220 -1.332305 -0.597572  ...  0.479183 -0.414356   \n",
       "pitiful     -0.095891 -0.227439 -0.187051 -1.081465  ...  0.064858  1.773807   \n",
       "nerd         0.120520 -0.288433  1.718499 -0.960887  ... -0.314480 -0.817938   \n",
       "excellent   -0.884837 -0.551854  0.002331  1.136410  ...  0.568573  0.601096   \n",
       "work        -0.243579 -1.743586 -1.012346  0.902370  ... -1.034917  1.247433   \n",
       "supreme     -1.499792  0.452104 -0.106776  0.279510  ... -0.007572 -0.781752   \n",
       "quality     -0.140252 -0.226453 -0.284805  0.890681  ...  1.266460  0.067776   \n",
       "bad          0.628805  2.609555  0.374878  0.466748  ...  0.659550 -0.984129   \n",
       "highly       1.282865 -1.971238  0.286252 -0.012342  ... -0.186937  0.070617   \n",
       "respectable  0.576895 -1.175409  0.923695 -0.620383  ...  0.703573 -1.360878   \n",
       "\n",
       "                   92        93        94        95        96        97  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>       -0.994724 -1.118804  1.153105 -0.068381  0.769769 -0.516143   \n",
       "nice        -0.169070  0.885088 -1.246579  0.048523 -0.445946  0.404705   \n",
       "great        0.460028 -0.822666 -0.853711 -0.232818  1.406272 -0.409277   \n",
       "best        -2.392415  0.597881  1.046835 -0.689517  0.501067 -0.220726   \n",
       "amazing     -0.357278 -0.204549  0.989971 -1.906618 -0.715281 -0.508421   \n",
       "stop         0.916183 -0.624366  1.730734  1.245013 -0.740861 -0.352895   \n",
       "lies         0.223169 -1.400546  1.207151  0.776599 -0.996237  0.711294   \n",
       "pitiful     -1.998466  0.394613  0.139130  1.054239 -0.806519  1.227212   \n",
       "nerd         1.107119  0.017147  0.990949  0.505777 -1.218458  1.140192   \n",
       "excellent   -0.379181  0.106090 -0.318359 -2.015614 -1.582134  3.208747   \n",
       "work         3.093121 -0.385246  0.075501 -0.557319  0.571115 -0.074529   \n",
       "supreme      0.889872 -1.179870 -0.792910 -1.446900  0.411810 -0.296746   \n",
       "quality      0.134996 -1.359200 -0.303195  0.526164 -0.502707  0.582444   \n",
       "bad         -1.071270 -0.897544 -0.001685 -0.002541 -1.872608 -2.139664   \n",
       "highly       0.112873  0.387419  1.348049  1.519517  0.293803 -1.134734   \n",
       "respectable  0.361350 -0.235250 -1.195826 -0.241891 -1.333441 -0.243081   \n",
       "\n",
       "                   98        99  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        1.574835 -0.292705  \n",
       "nice         0.059955 -1.336465  \n",
       "great       -3.135439  0.809925  \n",
       "best         1.172062 -0.765700  \n",
       "amazing      0.664267  1.338793  \n",
       "stop         0.413123  0.835810  \n",
       "lies         0.116161  0.337716  \n",
       "pitiful     -1.568208  3.540898  \n",
       "nerd        -0.387397 -0.013644  \n",
       "excellent   -0.716470  1.740199  \n",
       "work        -0.230721  1.586959  \n",
       "supreme     -1.426191  1.423158  \n",
       "quality      0.485411 -1.450893  \n",
       "bad          0.093976 -1.391590  \n",
       "highly      -0.881298  0.300755  \n",
       "respectable  0.609667 -0.180270  \n",
       "\n",
       "[17 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 가중치 확인 : 학습 전/후 Embedding 테이블과 단어별 벡터 조회\n",
    "import pandas as pd\n",
    "\n",
    "# 학습 전 임벧ㅇ 벡터\n",
    "wv = model.embedding.weight.data    # Embedding 층의 가중치 행렬(단어ID x 임베딩 차원) 추출\n",
    "print(wv.shape) # (vocab_size, embedding_dim)\n",
    "\n",
    "# 특정 단어 벡터\n",
    "vocab = word_to_index.keys()    # 단어사전에서 단어만 뽑아온다.\n",
    "pd.DataFrame(wv,index=vocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b42e25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 학습 준비 : 텐서 변환 -> DataLoader 구성 -> 손실함수/옵티마이저 설정\n",
    "X = torch.tensor(padded_sequnces, dtype=torch.long)         # 입력 시퀀스(정수 ID)를 LongTensor로 변환\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)    # 라벨을 float으로 변환 후 (N,) -> (N,1)로 차원 맞춤\n",
    "\n",
    "dataset = TensorDataset(X,y)\n",
    "dataLoader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "criterion=nn.BCEWithLogitsLoss()                       # 출력 logit과 정답(0/1)로 이진분류 손실 계산(시그모이드 포함)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d929d",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss을 사용할 떄에는 모델 출력이 sigmoid를 거치지않은 logit이어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fe795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.6482200622558594\n",
      "Epoch 2: loss 0.5331729203462601\n",
      "Epoch 3: loss 0.4244436025619507\n",
      "Epoch 4: loss 0.3081277571618557\n",
      "Epoch 5: loss 0.21190485171973705\n",
      "Epoch 6: loss 0.1461825855076313\n",
      "Epoch 7: loss 0.10756273940205574\n",
      "Epoch 8: loss 0.08008461818099022\n",
      "Epoch 9: loss 0.062396758235991\n",
      "Epoch 10: loss 0.04782864544540644\n",
      "Epoch 11: loss 0.03916932921856642\n",
      "Epoch 12: loss 0.0340281967073679\n",
      "Epoch 13: loss 0.02790939388796687\n",
      "Epoch 14: loss 0.02527895336970687\n",
      "Epoch 15: loss 0.02127641998231411\n",
      "Epoch 16: loss 0.018977869069203734\n",
      "Epoch 17: loss 0.017061745980754495\n",
      "Epoch 18: loss 0.0156174311414361\n",
      "Epoch 19: loss 0.014366717310622334\n",
      "Epoch 20: loss 0.013972284272313118\n"
     ]
    }
   ],
   "source": [
    "# 학습루프 : 미니배치 단위로 20 epoch 학습하여 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss=0    # 손실 누적\n",
    "    \n",
    "    for x_batch, y_batch in dataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)             # 순전파로 logit 계ㅏㄴ\n",
    "        loss = criterion(output, y_batch)   # 예측LOGIT과 정답으로 손실 계산\n",
    "        loss.backward()                     # 역전파로 기울기 계산\n",
    "        optimizer.step()                    # 파라미터 업뎅트\n",
    "        \n",
    "        epoch_loss += loss.item()   # 배치손실을 float로 누적\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: loss {epoch_loss / len(dataLoader)}\")  # epoch별 편균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8df694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 계산\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "    \n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168dbed",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b675a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "model_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ba8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "(17, 300)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 매트릭스 초기화 : 사전학습 벡터로 Embedding 레이어를 채우기 위한 준비\n",
    "print(len(word_to_index))\n",
    "\n",
    "# (vocab_size, embedding_dim) 크기의 0 행렬 생성\n",
    "embedding_matrix = np.zeros((len(word_to_index), model_wv.vectors.shape[1]))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d541055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 임베딩 매핑 : 내 단어사전을 GoogleNews 벡터로 채워 embedding_matrix 구성\n",
    "# model_wv.key_to_index['bad']    # 'bad'의 내부 인덱스 확인(706)\n",
    "# model_wv.vectors[240]\n",
    "\n",
    "# 단어가 사전학습 모델에 있으면 임베딩 벡터(np.ndarray)를 반환, 없으면 None 반환\n",
    "def get_word_embedding(word):\n",
    "    if word in model_wv:        # 사전학습 단어가 존재하면\n",
    "        return model_wv[word]   # 해당 단어 임베딩 벡터 반환\n",
    "    else:\n",
    "        return None\n",
    "# get_word_embedding('bad')\n",
    "for word, index in word_to_index.items():   # 내 단어사전(단어 -> 인덱스)를 순회\n",
    "    if index >= 2:                          # 특수토큰 제외\n",
    "        emb = get_word_embedding(word)      # 사전학습 임베딩ㅇ서 해당 단어 벡터 조회\n",
    "        if emb is not None:                 # 벡터가 존재하면\n",
    "            embedding_matrix[index] = emb   # 내 인덱스 위치에 사전학습 벡터를 복사해서 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcc3bb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.213867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>-0.021240</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.157227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.091309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.014099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice         0.158203  0.105957 -0.189453  0.386719  0.083496 -0.267578   \n",
       "great        0.071777  0.208008 -0.028442  0.178711  0.132812 -0.099609   \n",
       "best        -0.126953  0.021973  0.287109  0.153320  0.127930  0.032715   \n",
       "amazing      0.073730  0.004059 -0.135742  0.022095  0.180664 -0.046631   \n",
       "stop        -0.057861  0.013184  0.115234  0.069824 -0.306641 -0.044678   \n",
       "lies         0.149414 -0.012817  0.328125  0.025513  0.017334  0.190430   \n",
       "pitiful      0.269531  0.253906 -0.020996  0.060303 -0.010925  0.217773   \n",
       "nerd         0.265625 -0.207031 -0.026611  0.419922 -0.208984  0.390625   \n",
       "excellent   -0.212891 -0.004303 -0.180664 -0.007568  0.112793  0.163086   \n",
       "work        -0.075684  0.033691 -0.064941  0.131836  0.050537  0.149414   \n",
       "supreme      0.173828  0.172852  0.112793  0.166016  0.058594 -0.014221   \n",
       "quality     -0.259766  0.271484  0.119629  0.007233  0.057373  0.113770   \n",
       "bad          0.062988  0.124512  0.113281  0.073242  0.038818  0.079102   \n",
       "highly       0.050781 -0.227539 -0.130859  0.062500 -0.165039  0.118652   \n",
       "respectable  0.150391  0.285156 -0.023193  0.095703 -0.022095  0.058350   \n",
       "\n",
       "                  6         7         8         9    ...       290       291  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "nice         0.083496  0.113281 -0.104004  0.178711  ... -0.085449  0.189453   \n",
       "great        0.096191 -0.116699 -0.008545  0.148438  ... -0.011475  0.064453   \n",
       "best        -0.115723 -0.029541  0.153320  0.011292  ...  0.006439 -0.033936   \n",
       "amazing      0.224609 -0.229492 -0.040039  0.225586  ...  0.018433 -0.021240   \n",
       "stop         0.048584  0.152344  0.073242 -0.100098  ...  0.100098  0.171875   \n",
       "lies         0.188477 -0.143555 -0.090820  0.206055  ... -0.308594  0.183594   \n",
       "pitiful      0.139648 -0.057617  0.312500  0.253906  ... -0.063477  0.132812   \n",
       "nerd         0.164062  0.063965  0.149414 -0.017700  ...  0.215820  0.125000   \n",
       "excellent   -0.014709 -0.078613 -0.164062  0.279297  ...  0.136719  0.000282   \n",
       "work         0.109375 -0.133789 -0.020874  0.054688  ... -0.187500  0.101562   \n",
       "supreme      0.128906 -0.217773  0.073730  0.205078  ...  0.140625 -0.221680   \n",
       "quality      0.166992  0.025024  0.067871  0.120117  ... -0.145508  0.120117   \n",
       "bad          0.050781  0.171875  0.096191  0.220703  ...  0.011353  0.341797   \n",
       "highly      -0.230469 -0.225586  0.245117 -0.086914  ... -0.013733 -0.013489   \n",
       "respectable -0.155273 -0.051025  0.100098  0.001183  ...  0.058838  0.056885   \n",
       "\n",
       "                  292       293       294       295       296       297  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice        -0.146484  0.134766 -0.040771  0.032715  0.089355 -0.267578   \n",
       "great       -0.289062 -0.048096 -0.199219 -0.071289  0.064453 -0.167969   \n",
       "best        -0.166016 -0.016846 -0.048584 -0.022827 -0.152344 -0.101562   \n",
       "amazing     -0.250000 -0.020142 -0.310547 -0.207031 -0.006317 -0.141602   \n",
       "stop        -0.113281  0.064453 -0.115723  0.048096 -0.004822  0.086426   \n",
       "lies        -0.202148  0.031494 -0.164062 -0.201172  0.080078 -0.105469   \n",
       "pitiful     -0.094238  0.089355 -0.065430 -0.016235 -0.107910 -0.072266   \n",
       "nerd        -0.227539 -0.310547 -0.112793 -0.096680  0.255859  0.124023   \n",
       "excellent   -0.173828  0.004242 -0.081055  0.013550 -0.008362 -0.129883   \n",
       "work        -0.091309  0.052246 -0.164062  0.121582  0.062500  0.012024   \n",
       "supreme     -0.055664  0.034424 -0.119629 -0.081543  0.121094 -0.164062   \n",
       "quality     -0.314453  0.022095 -0.010254  0.298828  0.046387 -0.179688   \n",
       "bad         -0.090332  0.076660 -0.032471  0.133789 -0.154297 -0.063477   \n",
       "highly       0.175781  0.226562  0.086914 -0.210938 -0.111816 -0.056641   \n",
       "respectable -0.187500  0.138672 -0.227539  0.183594 -0.033447 -0.200195   \n",
       "\n",
       "                  298       299  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.000000  0.000000  \n",
       "nice         0.008362 -0.213867  \n",
       "great       -0.020874 -0.142578  \n",
       "best        -0.090332  0.088379  \n",
       "amazing     -0.150391 -0.137695  \n",
       "stop         0.029907  0.007812  \n",
       "lies         0.149414  0.157227  \n",
       "pitiful     -0.094238  0.028809  \n",
       "nerd        -0.030273  0.082031  \n",
       "excellent   -0.215820  0.012268  \n",
       "work         0.135742 -0.091309  \n",
       "supreme     -0.127930  0.097656  \n",
       "quality      0.000706 -0.014099  \n",
       "bad          0.114746  0.031006  \n",
       "highly       0.102539  0.074707  \n",
       "respectable -0.112305  0.103027  \n",
       "\n",
       "[17 rows x 300 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix, index=word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e4a28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (embedding): Embedding(17, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)ㄹㄹ 출력\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,    # 임베딩 차원\n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # 사전학습된 임베딩 벡터로 chrlghk : Embedding 가중치를 사전학습 행렬로 덮어쓰기\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float))\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim,hidden_size,batch_first=True)   # 입력(배치,길이.차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size,1)     # 맞막 은닉 상태를 1차원 logit으로 변환\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)    # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)   # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))  # (batch_size, hidden_size) -> (batch,1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = model_wv.vectors.shape[1]   # 사전학습 임베딩 차원 (300)으로 임베딩 차원 설정\n",
    "model = SimpleNet(vocab_size,embedding_dim,hidden_size=16)  # 어휘크기/임베딩차원/은닉크기로 모델 생성\n",
    "print(model)\n",
    "\n",
    "criterion=nn.BCEWithLogitsLoss()                       # 출력 logit과 정답(0/1)로 이진분류 손실 계산(시그모이드 포함)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73c33c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.7069666683673859\n",
      "Epoch 2: loss 0.5465743914246559\n",
      "Epoch 3: loss 0.5055489987134933\n",
      "Epoch 4: loss 0.39554160088300705\n",
      "Epoch 5: loss 0.2865040823817253\n",
      "Epoch 6: loss 0.22097426652908325\n",
      "Epoch 7: loss 0.16070615500211716\n",
      "Epoch 8: loss 0.11651797406375408\n",
      "Epoch 9: loss 0.08373840525746346\n",
      "Epoch 10: loss 0.06633596867322922\n",
      "Epoch 11: loss 0.05124673433601856\n",
      "Epoch 12: loss 0.04191670008003712\n",
      "Epoch 13: loss 0.032123691868036985\n",
      "Epoch 14: loss 0.028226724360138178\n",
      "Epoch 15: loss 0.023623700253665447\n",
      "Epoch 16: loss 0.021075201220810413\n",
      "Epoch 17: loss 0.01825667219236493\n",
      "Epoch 18: loss 0.016674587270244956\n",
      "Epoch 19: loss 0.014839658979326487\n",
      "Epoch 20: loss 0.013508475618436933\n"
     ]
    }
   ],
   "source": [
    "# 학습루프 : 미니배치 단위로 20 epoch 학습하여 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss=0    # 손실 누적\n",
    "    \n",
    "    for x_batch, y_batch in dataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)             # 순전파로 logit 계ㅏㄴ\n",
    "        loss = criterion(output, y_batch)   # 예측LOGIT과 정답으로 손실 계산\n",
    "        loss.backward()                     # 역전파로 기울기 계산\n",
    "        optimizer.step()                    # 파라미터 업뎅트\n",
    "        \n",
    "        epoch_loss += loss.item()   # 배치손실을 float로 누적\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: loss {epoch_loss / len(dataLoader)}\")  # epoch별 편균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3784ed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 계산\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "    \n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ff839",
   "metadata": {},
   "source": [
    "사전학습 임베딩을 사용했을 때에도 학습 데이터가 분류가 잘 되는지 파악한다.  \n",
    "만약 틀린 샘플이 있다면 해당 문장이 OOV(0벡터) 비중이 큰지 확인해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344150f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacd7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
