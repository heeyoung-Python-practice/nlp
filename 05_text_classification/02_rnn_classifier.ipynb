{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbcIPWYG4Qj6"
   },
   "source": [
    "# RNN기반 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1750140631111,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "6U_oVjNc4c3I",
    "outputId": "339f9151-f4ae-4015-8be5-784639f4fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'rec.sport.baseball', 'sci.space']\n",
      "From: kjenks@gothamcity.jsc.nasa.gov\n",
      "Subject: Life on Mars???\n",
      "Organization: NASA/JSC/GM2, Space Shuttle Program Office \n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "Lines: 12\n",
      "\n",
      "I know it's only wishful thinking, with our current President,\n",
      "but this is from last fall:\n",
      "\n",
      "     \"Is there life on Mars?  Maybe not now.  But there will be.\"\n",
      "        -- Daniel S. Goldin, NASA Administrator, 24 August 1992\n",
      "\n",
      "-- Ken Jenks, NASA/JSC/GM2, Space Shuttle Program Office\n",
      "      kjenks@gothamcity.jsc.nasa.gov  (713) 483-4368\n",
      "\n",
      "     \"The man who makes no mistakes does not usually make\n",
      "      anything.\"\n",
      "        -- Edward John Phelps, American Diplomat/Lawyer (1825-1895)\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "print(newsgroups.target_names)\n",
    "print(X[0])\n",
    "print(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1750140647214,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "yXBtkCIp4H51",
    "outputId": "e593d05d-06d7-4379-efa1-6ddb83765ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2954, 200)\n"
     ]
    }
   ],
   "source": [
    "# 데이터전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X)   # 토크나이저 단어 사전 학습\n",
    "X_encoded = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_encoded, maxlen=max_len) # 길이를 max_len으로 통일(패딩/자르기)\n",
    "print(X_padded.shape)   #(문서수,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0suBZZ3V6ZuL"
   },
   "outputs": [],
   "source": [
    "# 데이터분리/텐서변환\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 전체데이터에서 train/valid.test로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(torch.tensor(X_padded), torch.tensor(y), test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset/dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7BjT5FY8Tnt"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "\n",
    "# 임베딩 -> LSTM -< FC로 문장을 요약해 3개 클래스 로짓을 출력하는 분류기 모델\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        # embedding - lstm - dense\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)   # (B,Y,*)\n",
    "        self.fc = nn.Linear(hidden_size, 3) # 마지막 은닉 -> 3개 클래스 로짓 반환\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)       # (B, T) -> (B, T, E)\n",
    "        _, (h, c) = self.lstm(x)    # h: (L, B, H), C: (L, B, H)\n",
    "        out = self.fc(h[-1])        # 마지막 레이어 은닉상태 (B, H) -> (B, 3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11441,
     "status": "ok",
     "timestamp": 1750139218556,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-vO1L30O8Vf5",
    "outputId": "3485c91a-9816-4db2-9410-6c819ee4562a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss 1.0565, Train Acc 0.4635, Val Loss 1.0042, Val Acc 0.5349, \n",
      "Epoch 2/50: Train Loss 0.9064, Train Acc 0.6032, Val Loss 0.8640, Val Acc 0.6195, \n",
      "Epoch 3/50: Train Loss 0.7147, Train Acc 0.7106, Val Loss 0.7429, Val Acc 0.7019, \n",
      "Epoch 4/50: Train Loss 0.5350, Train Acc 0.8011, Val Loss 0.7126, Val Acc 0.7252, \n",
      "Epoch 5/50: Train Loss 0.3922, Train Acc 0.8540, Val Loss 0.6159, Val Acc 0.7653, \n",
      "Epoch 6/50: Train Loss 0.2631, Train Acc 0.9085, Val Loss 0.5761, Val Acc 0.7780, \n",
      "Epoch 7/50: Train Loss 0.2181, Train Acc 0.9296, Val Loss 0.8745, Val Acc 0.7188, \n",
      "Epoch 8/50: Train Loss 0.2470, Train Acc 0.9196, Val Loss 0.5701, Val Acc 0.8076, \n",
      "Epoch 9/50: Train Loss 0.1068, Train Acc 0.9693, Val Loss 0.5674, Val Acc 0.8288, \n",
      "Epoch 10/50: Train Loss 0.0753, Train Acc 0.9820, Val Loss 0.6352, Val Acc 0.8330, \n",
      "Epoch 11/50: Train Loss 0.0519, Train Acc 0.9899, Val Loss 0.6052, Val Acc 0.8224, \n",
      "Epoch 12/50: Train Loss 0.0475, Train Acc 0.9926, Val Loss 0.7752, Val Acc 0.7780, \n",
      "Epoch 13/50: Train Loss 0.0607, Train Acc 0.9836, Val Loss 0.6260, Val Acc 0.8140, \n",
      "Epoch 14/50: Train Loss 0.0206, Train Acc 0.9974, Val Loss 0.6312, Val Acc 0.8161, \n",
      "Epoch 15/50: Train Loss 0.0142, Train Acc 0.9979, Val Loss 0.6127, Val Acc 0.8457, \n",
      "Epoch 16/50: Train Loss 0.0067, Train Acc 0.9995, Val Loss 0.6608, Val Acc 0.8372, \n",
      "Epoch 17/50: Train Loss 0.0111, Train Acc 0.9974, Val Loss 0.6836, Val Acc 0.8351, \n",
      "Epoch 18/50: Train Loss 0.0106, Train Acc 0.9979, Val Loss 0.6858, Val Acc 0.8330, \n",
      "Epoch 19/50: Train Loss 0.0071, Train Acc 0.9989, Val Loss 0.6724, Val Acc 0.8436, \n",
      "Epoch 20/50: Train Loss 0.0037, Train Acc 0.9995, Val Loss 0.6875, Val Acc 0.8520, \n",
      "Epoch 21/50: Train Loss 0.0026, Train Acc 0.9995, Val Loss 0.7236, Val Acc 0.8520, \n",
      "Epoch 22/50: Train Loss 0.0018, Train Acc 1.0000, Val Loss 0.7547, Val Acc 0.8520, \n",
      "Epoch 23/50: Train Loss 0.0012, Train Acc 1.0000, Val Loss 0.7352, Val Acc 0.8520, \n",
      "Epoch 24/50: Train Loss 0.0009, Train Acc 1.0000, Val Loss 0.7590, Val Acc 0.8478, \n",
      "Epoch 25/50: Train Loss 0.0008, Train Acc 1.0000, Val Loss 0.7618, Val Acc 0.8541, \n",
      "Epoch 26/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.7946, Val Acc 0.8541, \n",
      "Epoch 27/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.8021, Val Acc 0.8562, \n",
      "Epoch 28/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.8250, Val Acc 0.8562, \n",
      "Epoch 29/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.8324, Val Acc 0.8541, \n",
      "Epoch 30/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.8489, Val Acc 0.8541, \n",
      "Epoch 31/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.8593, Val Acc 0.8562, \n",
      "Epoch 32/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.8657, Val Acc 0.8584, \n",
      "Epoch 33/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.8762, Val Acc 0.8520, \n",
      "Epoch 34/50: Train Loss 0.0357, Train Acc 0.9889, Val Loss 0.7013, Val Acc 0.7780, \n",
      "Epoch 35/50: Train Loss 0.0894, Train Acc 0.9735, Val Loss 0.6833, Val Acc 0.7738, \n",
      "Epoch 36/50: Train Loss 0.0258, Train Acc 0.9952, Val Loss 0.7653, Val Acc 0.8118, \n",
      "Epoch 37/50: Train Loss 0.0074, Train Acc 0.9989, Val Loss 0.7457, Val Acc 0.8140, \n",
      "Epoch 38/50: Train Loss 0.0049, Train Acc 0.9989, Val Loss 0.7757, Val Acc 0.8245, \n",
      "Epoch 39/50: Train Loss 0.0027, Train Acc 0.9989, Val Loss 0.8158, Val Acc 0.8245, \n",
      "Epoch 40/50: Train Loss 0.0019, Train Acc 1.0000, Val Loss 0.8430, Val Acc 0.8309, \n",
      "Epoch 41/50: Train Loss 0.0027, Train Acc 0.9989, Val Loss 0.8941, Val Acc 0.8203, \n",
      "Epoch 42/50: Train Loss 0.0329, Train Acc 0.9889, Val Loss 0.7250, Val Acc 0.8140, \n",
      "Epoch 43/50: Train Loss 0.0151, Train Acc 0.9963, Val Loss 0.8994, Val Acc 0.7886, \n",
      "Epoch 44/50: Train Loss 0.0069, Train Acc 0.9979, Val Loss 0.8506, Val Acc 0.8013, \n",
      "Epoch 45/50: Train Loss 0.0028, Train Acc 0.9995, Val Loss 0.8490, Val Acc 0.8076, \n",
      "Epoch 46/50: Train Loss 0.0020, Train Acc 0.9995, Val Loss 0.8547, Val Acc 0.8161, \n",
      "Epoch 47/50: Train Loss 0.0018, Train Acc 0.9995, Val Loss 0.8658, Val Acc 0.8245, \n",
      "Epoch 48/50: Train Loss 0.0008, Train Acc 1.0000, Val Loss 0.8738, Val Acc 0.8372, \n",
      "Epoch 49/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.8849, Val Acc 0.8393, \n",
      "Epoch 50/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.8997, Val Acc 0.8372, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda 또는 cpu\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습루프\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()\n",
    "        pred = output.argmax(dim=1) # 가장 큰 로짓 인덱스로 반환\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "        train_total += len(y_batch)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "            val_total += len(y_batch)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    # 출력(train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750139872430,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "clNCNefO9nBv",
    "outputId": "4505f72f-01c6-4123-81bc-30dc2c86a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.83      0.81      0.82       202\n",
      "rec.sport.baseball       0.84      0.88      0.86       202\n",
      "         sci.space       0.74      0.73      0.74       187\n",
      "\n",
      "          accuracy                           0.81       591\n",
      "         macro avg       0.81      0.81      0.81       591\n",
      "      weighted avg       0.81      0.81      0.81       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답, 모델 예측값을 가지고, classification_report 작성\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())       # 배치 예측을 리스트에 누적\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())   # 배치 정답을 리스트에 누적\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcaUXF_gGi-"
   },
   "source": [
    "## 사전학습된 임베딩 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22000,
     "status": "ok",
     "timestamp": 1750140554021,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "a9UBkjQlgQdh",
    "outputId": "2f5e4a97-1014-4754-f96f-9a54a3da4027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ted_en_fasttext.model : 모델 본체(메타데이터 + 학습된 파라미터 경로)\n",
    "- ted_en_fasttext.model.wv.vectors_ngrams.npy : FastText의 핵심 특징이 서브워드(n-gram)벡터 행렬(numpy 배열 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1750140871457,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "w8s1Ezq6hQlW",
    "outputId": "da2ef527-897d-4b87-ae81-c7ba2bb5980d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText.load('ted_en_fasttext.model')\n",
    "print(fasttext_model.vector_size)   # 단어 벡터 차원(단어 임베딩 벡터의 차원수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1750141482707,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "bv-YDGQJhyTf",
    "outputId": "aa62b304-44f1-45b2-c9ac-3a1094b38e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = fasttext_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) # (단어사전크기, 임베딩차원) 0 행렬 초기화\n",
    "\n",
    "word_index = tokenizer.word_index # 38000\n",
    "word_index = {word:index \\\n",
    "              for word, index in word_index.items() \\\n",
    "                if index < vocab_size}\n",
    "print(len(word_index)) # 10000\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if word in fasttext_model.wv:   # FastText가 해당 벡터를 가지고 있으면\n",
    "        embedding_matrix[index] = fasttext_model.wv[word]   # 해당 인덱스 위치에 임베딩 벡터를 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-wiQxp4Ag9xz"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "\n",
    "# 사전학습 임베딩(embedding_matrix)으로 임베딩 레이어를 초기화하면 LSTM 분류기\n",
    "class LSTMClassifier2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_size):\n",
    "        super().__init__()\n",
    "        # embedding - lstm - dense\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))    # 사전학습 임베딩 가중치로 초기화\n",
    "        self.embedding.weight.requires_grad = True  # 임베딩을 학습에 포함할지 여부 (True = 미세조정, False 고정)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)       # (B, T) -> (B, T, E)\n",
    "        _, (h, c) = self.lstm(x)    # h: (L, B, H)\n",
    "        out = self.fc(h[-1])        # 마지막 레이어 ㅇㄴ닉 사용 (B,H) -> (B,3)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23895,
     "status": "ok",
     "timestamp": 1750142124548,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "47_k6ozcg9x1",
    "outputId": "c2d0c0d9-98bb-40f9-bf06-bf56a79d8627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss 1.0972, Train Acc 0.3550, Val Loss 1.0947, Val Acc 0.3890, \n",
      "Epoch 2/100: Train Loss 1.0939, Train Acc 0.3794, Val Loss 1.0917, Val Acc 0.4186, \n",
      "Epoch 3/100: Train Loss 1.0905, Train Acc 0.4249, Val Loss 1.0877, Val Acc 0.4567, \n",
      "Epoch 4/100: Train Loss 1.0866, Train Acc 0.4556, Val Loss 1.0830, Val Acc 0.4926, \n",
      "Epoch 5/100: Train Loss 1.0812, Train Acc 0.4984, Val Loss 1.0769, Val Acc 0.5095, \n",
      "Epoch 6/100: Train Loss 1.0723, Train Acc 0.4857, Val Loss 1.0627, Val Acc 0.4968, \n",
      "Epoch 7/100: Train Loss 1.0305, Train Acc 0.5153, Val Loss 0.9507, Val Acc 0.6068, \n",
      "Epoch 8/100: Train Loss 0.9351, Train Acc 0.5852, Val Loss 0.9151, Val Acc 0.5962, \n",
      "Epoch 9/100: Train Loss 0.8802, Train Acc 0.6175, Val Loss 0.9326, Val Acc 0.5835, \n",
      "Epoch 10/100: Train Loss 0.9645, Train Acc 0.5122, Val Loss 1.1734, Val Acc 0.3129, \n",
      "Epoch 11/100: Train Loss 1.1079, Train Acc 0.3646, Val Loss 1.1199, Val Acc 0.3636, \n",
      "Epoch 12/100: Train Loss 1.0684, Train Acc 0.4206, Val Loss 1.0865, Val Acc 0.4271, \n",
      "Epoch 13/100: Train Loss 1.0443, Train Acc 0.4730, Val Loss 1.0625, Val Acc 0.4545, \n",
      "Epoch 14/100: Train Loss 1.0198, Train Acc 0.5132, Val Loss 1.0330, Val Acc 0.5011, \n",
      "Epoch 15/100: Train Loss 0.9562, Train Acc 0.6370, Val Loss 0.8661, Val Acc 0.6786, \n",
      "Epoch 16/100: Train Loss 0.8296, Train Acc 0.7000, Val Loss 0.8073, Val Acc 0.7019, \n",
      "Epoch 17/100: Train Loss 0.7888, Train Acc 0.7381, Val Loss 0.7739, Val Acc 0.6977, \n",
      "Epoch 18/100: Train Loss 0.7513, Train Acc 0.7407, Val Loss 0.7515, Val Acc 0.6871, \n",
      "Epoch 19/100: Train Loss 0.7280, Train Acc 0.7402, Val Loss 0.7350, Val Acc 0.6934, \n",
      "Epoch 20/100: Train Loss 0.7008, Train Acc 0.7275, Val Loss 0.7113, Val Acc 0.6977, \n",
      "Epoch 21/100: Train Loss 0.6632, Train Acc 0.7529, Val Loss 0.6923, Val Acc 0.7230, \n",
      "Epoch 22/100: Train Loss 0.6110, Train Acc 0.7683, Val Loss 0.6639, Val Acc 0.7526, \n",
      "Epoch 23/100: Train Loss 0.5690, Train Acc 0.7979, Val Loss 0.7688, Val Acc 0.7146, \n",
      "Epoch 24/100: Train Loss 0.6171, Train Acc 0.7635, Val Loss 0.6177, Val Acc 0.7378, \n",
      "Epoch 25/100: Train Loss 0.4999, Train Acc 0.8365, Val Loss 0.5278, Val Acc 0.7949, \n",
      "Epoch 26/100: Train Loss 0.5434, Train Acc 0.7524, Val Loss 0.6275, Val Acc 0.7611, \n",
      "Epoch 27/100: Train Loss 0.4876, Train Acc 0.8206, Val Loss 0.5228, Val Acc 0.8034, \n",
      "Epoch 28/100: Train Loss 0.4029, Train Acc 0.8677, Val Loss 0.4657, Val Acc 0.8203, \n",
      "Epoch 29/100: Train Loss 0.3450, Train Acc 0.8857, Val Loss 0.4475, Val Acc 0.8140, \n",
      "Epoch 30/100: Train Loss 0.3129, Train Acc 0.8974, Val Loss 0.4124, Val Acc 0.8414, \n",
      "Epoch 31/100: Train Loss 0.2795, Train Acc 0.9106, Val Loss 0.4377, Val Acc 0.8076, \n",
      "Epoch 32/100: Train Loss 0.2926, Train Acc 0.8947, Val Loss 0.3937, Val Acc 0.8372, \n",
      "Epoch 33/100: Train Loss 0.2518, Train Acc 0.9249, Val Loss 0.3726, Val Acc 0.8541, \n",
      "Epoch 34/100: Train Loss 0.2733, Train Acc 0.8989, Val Loss 0.4295, Val Acc 0.8224, \n",
      "Epoch 35/100: Train Loss 0.2241, Train Acc 0.9354, Val Loss 0.3668, Val Acc 0.8605, \n",
      "Epoch 36/100: Train Loss 0.1811, Train Acc 0.9450, Val Loss 0.4021, Val Acc 0.8647, \n",
      "Epoch 37/100: Train Loss 0.1841, Train Acc 0.9429, Val Loss 0.4460, Val Acc 0.8478, \n",
      "Epoch 38/100: Train Loss 0.1644, Train Acc 0.9524, Val Loss 0.3470, Val Acc 0.8689, \n",
      "Epoch 39/100: Train Loss 0.1590, Train Acc 0.9534, Val Loss 0.6200, Val Acc 0.8182, \n",
      "Epoch 40/100: Train Loss 0.1727, Train Acc 0.9466, Val Loss 0.3387, Val Acc 0.8710, \n",
      "Epoch 41/100: Train Loss 0.1451, Train Acc 0.9566, Val Loss 0.2950, Val Acc 0.9049, \n",
      "Epoch 42/100: Train Loss 0.1296, Train Acc 0.9603, Val Loss 0.5185, Val Acc 0.8584, \n",
      "Epoch 43/100: Train Loss 0.2626, Train Acc 0.9286, Val Loss 0.4029, Val Acc 0.8668, \n",
      "Epoch 44/100: Train Loss 0.1281, Train Acc 0.9672, Val Loss 0.3001, Val Acc 0.9070, \n",
      "Epoch 45/100: Train Loss 0.1034, Train Acc 0.9788, Val Loss 0.3028, Val Acc 0.8964, \n",
      "Epoch 46/100: Train Loss 0.1050, Train Acc 0.9725, Val Loss 0.2752, Val Acc 0.9027, \n",
      "Epoch 47/100: Train Loss 0.1085, Train Acc 0.9720, Val Loss 0.2922, Val Acc 0.9027, \n",
      "Epoch 48/100: Train Loss 0.0825, Train Acc 0.9794, Val Loss 0.2896, Val Acc 0.9027, \n",
      "Epoch 49/100: Train Loss 0.0774, Train Acc 0.9825, Val Loss 0.2905, Val Acc 0.9049, \n",
      "Epoch 50/100: Train Loss 0.0718, Train Acc 0.9873, Val Loss 0.3004, Val Acc 0.8943, \n",
      "Epoch 51/100: Train Loss 0.0771, Train Acc 0.9799, Val Loss 0.2757, Val Acc 0.9070, \n",
      "Epoch 52/100: Train Loss 0.0567, Train Acc 0.9894, Val Loss 0.3553, Val Acc 0.8964, \n",
      "Epoch 53/100: Train Loss 0.0607, Train Acc 0.9862, Val Loss 0.2407, Val Acc 0.9281, \n",
      "Epoch 54/100: Train Loss 0.0533, Train Acc 0.9905, Val Loss 0.2183, Val Acc 0.9323, \n",
      "Epoch 55/100: Train Loss 0.0498, Train Acc 0.9894, Val Loss 0.2735, Val Acc 0.9175, \n",
      "Epoch 56/100: Train Loss 0.0472, Train Acc 0.9894, Val Loss 0.2574, Val Acc 0.9260, \n",
      "Epoch 57/100: Train Loss 0.0659, Train Acc 0.9831, Val Loss 0.2606, Val Acc 0.9197, \n",
      "Epoch 58/100: Train Loss 0.0835, Train Acc 0.9799, Val Loss 0.2520, Val Acc 0.9154, \n",
      "Epoch 59/100: Train Loss 0.0628, Train Acc 0.9847, Val Loss 0.2415, Val Acc 0.9218, \n",
      "Epoch 60/100: Train Loss 0.0635, Train Acc 0.9841, Val Loss 0.2321, Val Acc 0.9197, \n",
      "Epoch 61/100: Train Loss 0.0702, Train Acc 0.9831, Val Loss 0.2366, Val Acc 0.9175, \n",
      "Epoch 62/100: Train Loss 0.0371, Train Acc 0.9931, Val Loss 0.2427, Val Acc 0.9197, \n",
      "Epoch 63/100: Train Loss 0.0318, Train Acc 0.9937, Val Loss 0.2869, Val Acc 0.9133, \n",
      "Epoch 64/100: Train Loss 0.0287, Train Acc 0.9963, Val Loss 0.3031, Val Acc 0.9112, \n",
      "Epoch 65/100: Train Loss 0.0248, Train Acc 0.9958, Val Loss 0.3071, Val Acc 0.9133, \n",
      "Epoch 66/100: Train Loss 0.0354, Train Acc 0.9921, Val Loss 0.2629, Val Acc 0.9218, \n",
      "Epoch 67/100: Train Loss 0.3564, Train Acc 0.9042, Val Loss 0.7338, Val Acc 0.6998, \n",
      "Epoch 68/100: Train Loss 0.3025, Train Acc 0.9016, Val Loss 0.3665, Val Acc 0.8668, \n",
      "Epoch 69/100: Train Loss 0.1584, Train Acc 0.9503, Val Loss 0.3100, Val Acc 0.9091, \n",
      "Epoch 70/100: Train Loss 0.0646, Train Acc 0.9820, Val Loss 0.2178, Val Acc 0.9260, \n",
      "Epoch 71/100: Train Loss 0.0475, Train Acc 0.9884, Val Loss 0.2274, Val Acc 0.9302, \n",
      "Epoch 72/100: Train Loss 0.0356, Train Acc 0.9947, Val Loss 0.2230, Val Acc 0.9323, \n",
      "Epoch 73/100: Train Loss 0.0301, Train Acc 0.9963, Val Loss 0.2759, Val Acc 0.9260, \n",
      "Epoch 74/100: Train Loss 0.0290, Train Acc 0.9952, Val Loss 0.2563, Val Acc 0.9260, \n",
      "Epoch 75/100: Train Loss 0.0307, Train Acc 0.9963, Val Loss 0.2396, Val Acc 0.9281, \n",
      "Epoch 76/100: Train Loss 0.0227, Train Acc 0.9974, Val Loss 0.2464, Val Acc 0.9281, \n",
      "Epoch 77/100: Train Loss 0.0219, Train Acc 0.9974, Val Loss 0.2524, Val Acc 0.9281, \n",
      "Epoch 78/100: Train Loss 0.0207, Train Acc 0.9974, Val Loss 0.2486, Val Acc 0.9302, \n",
      "Epoch 79/100: Train Loss 0.0232, Train Acc 0.9963, Val Loss 0.2408, Val Acc 0.9239, \n",
      "Epoch 80/100: Train Loss 0.0319, Train Acc 0.9947, Val Loss 0.2469, Val Acc 0.9197, \n",
      "Epoch 81/100: Train Loss 0.0986, Train Acc 0.9683, Val Loss 0.3618, Val Acc 0.8626, \n",
      "Epoch 82/100: Train Loss 0.0848, Train Acc 0.9725, Val Loss 0.2180, Val Acc 0.9218, \n",
      "Epoch 83/100: Train Loss 0.0262, Train Acc 0.9963, Val Loss 0.2425, Val Acc 0.9281, \n",
      "Epoch 84/100: Train Loss 0.0233, Train Acc 0.9974, Val Loss 0.2461, Val Acc 0.9239, \n",
      "Epoch 85/100: Train Loss 0.0203, Train Acc 0.9984, Val Loss 0.2363, Val Acc 0.9281, \n",
      "Epoch 86/100: Train Loss 0.0203, Train Acc 0.9979, Val Loss 0.2400, Val Acc 0.9260, \n",
      "Epoch 87/100: Train Loss 0.0184, Train Acc 0.9979, Val Loss 0.2514, Val Acc 0.9218, \n",
      "Epoch 88/100: Train Loss 0.0184, Train Acc 0.9979, Val Loss 0.2555, Val Acc 0.9281, \n",
      "Epoch 89/100: Train Loss 0.0175, Train Acc 0.9979, Val Loss 0.2478, Val Acc 0.9260, \n",
      "Epoch 90/100: Train Loss 0.0167, Train Acc 0.9979, Val Loss 0.2630, Val Acc 0.9302, \n",
      "Epoch 91/100: Train Loss 0.0149, Train Acc 0.9984, Val Loss 0.2461, Val Acc 0.9281, \n",
      "Epoch 92/100: Train Loss 0.0135, Train Acc 0.9984, Val Loss 0.2493, Val Acc 0.9302, \n",
      "Epoch 93/100: Train Loss 0.0129, Train Acc 0.9984, Val Loss 0.2535, Val Acc 0.9302, \n",
      "Epoch 94/100: Train Loss 0.0175, Train Acc 0.9984, Val Loss 0.2874, Val Acc 0.9281, \n",
      "Epoch 95/100: Train Loss 0.0212, Train Acc 0.9958, Val Loss 0.3187, Val Acc 0.9154, \n",
      "Epoch 96/100: Train Loss 0.0257, Train Acc 0.9958, Val Loss 0.2293, Val Acc 0.9302, \n",
      "Epoch 97/100: Train Loss 0.0149, Train Acc 0.9974, Val Loss 0.2340, Val Acc 0.9302, \n",
      "Epoch 98/100: Train Loss 0.0168, Train Acc 0.9963, Val Loss 0.2523, Val Acc 0.9302, \n",
      "Epoch 99/100: Train Loss 0.0322, Train Acc 0.9937, Val Loss 0.9989, Val Acc 0.8414, \n",
      "Epoch 100/100: Train Loss 0.2583, Train Acc 0.9392, Val Loss 0.3174, Val Acc 0.9070, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda 또는 cpu\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = LSTMClassifier2(vocab_size, embedding_dim, embedding_matrix, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습루프\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "        train_total += len(y_batch)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "            val_total += len(y_batch)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    # 출력(train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750142126818,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-JkT4PSYg9x3",
    "outputId": "95531598-aa4b-4ba6-daa1-f95780ffa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.96      0.77      0.85       202\n",
      "rec.sport.baseball       0.85      0.98      0.91       202\n",
      "         sci.space       0.84      0.88      0.86       187\n",
      "\n",
      "          accuracy                           0.88       591\n",
      "         macro avg       0.88      0.88      0.88       591\n",
      "      weighted avg       0.89      0.88      0.88       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답, 모델 예측값을 가지고, classification_report 작성\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=newsgroups.target_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEca1JB5mJdMj6HFe4GY4d",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
