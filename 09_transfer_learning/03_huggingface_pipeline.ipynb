{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b7e722",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5483b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets       # Huggingface 모델/파이프라인 + 데이터셋 유틸\n",
    "%pip install -q sentencepiece               # 서브워드 토크나이저(SentencePiece)\n",
    "%pip install -q kobert-transformers         # koBERT 전용 transformer 래퍼\n",
    "%pip install -q python-dotenv               # .env (환경변수) 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34a8fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ccf32",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "주어진 task에서 사전 훈련된 모델을 사용하는 가장 간단한 방법은 `pipeline`을 사용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "605f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline   # HuggingFace 파이프라인 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9297100",
   "metadata": {},
   "source": [
    "pipeline() : 항상 Hugging Face Hub에 있는 모델을 기본으로 사용\n",
    "- 모델을 명시하지 않는 경우\n",
    "    - Huggingface가 task별 기본(default) 모델을 자동 선택\n",
    "    - 이 기본모델은 HuggingFace Hub에 공개된 모델\n",
    "- 모델을 명시한 경우\n",
    "    - Huggingface에 올라온 특정 repo에서 다운로드\n",
    "- 모델 객체를 직접 넘길 경우\n",
    "    - 로컬에서 로드\n",
    "    - 직접 학습/파인튜닝한 모델\n",
    "    - 이런 경우에는 pipeline이 \"추론 레퍼\"의 역할만 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9af30",
   "metadata": {},
   "source": [
    "##### 기계번역\n",
    "\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccd73e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.4.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "296d4d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x22c83dcb8c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 -> 영어\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-ko-en')\n",
    "translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f52e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Charge to the hot dog!'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('핫도그를 향하여 돌격!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "820d0eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'I hope you study hard.'},\n",
       " {'translation_text': \"I'd like you to take the class really hard.\"},\n",
       " {'translation_text': \"I'd like you to work hard on your practice.\"},\n",
       " {'translation_text': \"Let's work hard not to forget the coding test.\"},\n",
       " {'translation_text': \"Let's all succeed!\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator([\n",
    "    '공부를 열심히 하시면 좋겠어요.',\n",
    "    '수업을 열심히 들으시면 좋겠고요.',\n",
    "    '실습도 열심히 해주시면 좋겠어요.',\n",
    "    '코딩테스트도 까먹지말고 열심히 합시다.',\n",
    "    '모두 성공합시다!'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ea992",
   "metadata": {},
   "source": [
    "##### 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e2cfbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89d7c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8970896601676941}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I'm the worst and I like it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33cbfd",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4d084c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_sentiment_clf = pipeline('sentiment-analysis', model='sangrimlee/bert-base-multilingual-cased-nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "706ac9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9047993421554565},\n",
       " {'label': 'negative', 'score': 0.7013086676597595},\n",
       " {'label': 'positive', 'score': 0.7580994367599487}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_sentiment_clf([\n",
    "    '어제 배운 Attention이 어려워서 맥주를 한 잔 했어.',\n",
    "    '공부는 어렵지만 맥주는 시원하더라',\n",
    "    '하지만 나는 굴하지 않고 열심히 공부할거야!!!'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaec0cf",
   "metadata": {},
   "source": [
    "##### Zero Shot Classification\n",
    "\n",
    "- shot == 예시\n",
    "- 예시 없이 (학습 없이) 추론(분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b98204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 제로샷 분류 파이프라인 (기본 NLI 모델 자동 로드)\n",
    "zero_shot_clf = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f843b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9053581953048706, 0.07259626686573029, 0.022045595571398735]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"This is a course about the transformers library.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f7a260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"It has a poison and it's very dangerous\",\n",
       " 'labels': ['snake', 'tiger', 'squirrel'],\n",
       " 'scores': [0.6373922228813171, 0.19870398938655853, 0.16390381753444672]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"It has a poison and it's very dangerous\",\n",
    "    candidate_labels=[\"tiger\", \"squirrel\", \"snake\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f55e07",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/joeddav/xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf790bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--joeddav--xlm-roberta-large-xnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 다국어(XNLI) 기반 제로샷 분류모델\n",
    "ko_zero_shot_clf = pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e03551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025년에는 어떤 운동을 하시겠습니까?!',\n",
       " 'labels': ['정치', '건강', '경제'],\n",
       " 'scores': [0.40395545959472656, 0.3669034242630005, 0.22914111614227295]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025년에는 어떤 운동을 하시겠습니까?!\"  # 분류할 입력 문장\n",
    "candidate_labels = [\"정치\", \"경제\", \"건강\"]                     # 비교할 후보 라벨\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025년에는 어떤 운동을 하시겠습니까?!',\n",
       " 'labels': ['건강', '정치', '경제'],\n",
       " 'scores': [0.9117788672447205, 0.05924064293503761, 0.02898053266108036]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025년에는 어떤 운동을 하시겠습니까?!\"\n",
    "candidate_labels = [\"정치\", \"경제\", \"건강\"]\n",
    "hypothesis_template = \"이 텍스트는 {}에 관한 내용입니다.\"           # NLI 가설 문장 템플릿\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e06a3",
   "metadata": {},
   "source": [
    "##### 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ccf514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline('text-generation', model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f9d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the power of the word “” to describe the world in a way that only the world can understand.\\n\\n\\n\\nThe purpose of this course is to demonstrate the importance of using the word “” to describe the world in a way that only the world can understand.\\nIn this course we will cover how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use the word “” to describe the world in a way that only the world can understand.\\nIn this course we will show you how to use'},\n",
       " {'generated_text': 'In this course, we will teach you how to use the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of the power of'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\n",
    "    \"In this course, we will teach you how to\", # 텍스트 생성 시작 프롬프트\n",
    "    max_length=30,                              # 생성 결과 최대 토큰 길이\n",
    "    num_return_sequences=2                      # 서로 다른 생성 결과 2개 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fe4d",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbae860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--skt--ko-gpt-trinity-1.2B-v0.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_text_generator = pipeline('text-generation', model='skt/ko-gpt-trinity-1.2B-v0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a631f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"AI란 말입니다' 같은 문구는 '개인정보'라는 말에 힘을 싣고 있습니다. 개인정보란 말 그대로 개인의 모든 정보를 말합니다. 따라서 개인정보를 관리하고 제공하는 사람은 당연히 개인 정보를 보호해야 하고 그 정보를 제공받을 권리도 당연히 따라야 한다는 것이지요. 이런 맥락에서 '정보통신망 이용촉진 및 정보보호 등에 관한 법률'은 정보통신서비스 제공자에게 수집된 개인정보의 보호조치를 취하도록 하고 있습니다. \\n 그렇다면 '개인정보'라는 말은 언제부터 사용하게 됐을까요. 우리말로 하자면 '신용카드'와 '주민등록번호' 정도가 될 것 같습니다. '정보통신망 이용촉진 및 정보보호 등에 관한 법률'에서는 '개인정보'란 단어를 '개인정보주체에게 제공된 전체 또는 부분'이라고 정의하고 있습니다. 한마디로 '개인정보'란 말은 신용카드를 비롯해 각종 금융서비스, 주민등록번호, 휴대전화번호와 같이 정보통신서비스를 통해 제공된 모든 정보를 말합니다. \\n 그런데 여기서 짚고 넘어갈 것이 있습니다. '개인정보'란 단어에는 그 단어가 주는 뉘앙스가 있습니다. '신용카드'나 '주민등록번호'는 신용을 상징하는 단어입니다. 그런데 '개인정보'란 단어는 신용카드를 비롯해 금융서비스 등 각종 금융서비스를 통해 제공되는 모든 정보를 말합니다. 따라서 '개인정보'란 단어는 신용카드를 비롯해 금융서비스 등 각종 금융서비스를 통해 제공된 모든 정보를 말합니다. \\n 그러면\"}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_text_generator('AI란 말입니다')\n",
    "# ko_text_generator('오늘 점심에는')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abda36",
   "metadata": {},
   "source": [
    "##### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9248f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09073042869567871,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': 'This course will teach you all about business model.'},\n",
       " {'score': 0.072625532746315,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical model.'},\n",
       " {'score': 0.049304164946079254,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'This course will teach you all about the model.'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('This course will teach you all about <mask> model.', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902fc24",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\n",
    "    'ner', \n",
    "    model='dslim/bert-base-NER', \n",
    "    grouped_entities=True       # 연속된 동일 개체를 하나로 묶어서 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10a42578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.98995215),\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.46767735),\n",
       "  'word': '##qui',\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.72662497),\n",
       "  'word': '##rrel',\n",
       "  'start': 15,\n",
       "  'end': 19},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.986338),\n",
       "  'word': 'Playdata',\n",
       "  'start': 34,\n",
       "  'end': 42},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9994649),\n",
       "  'word': 'Seoul',\n",
       "  'start': 46,\n",
       "  'end': 51}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Squirrel and I work at Playdata in Seoul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999ca1d",
   "metadata": {},
   "source": [
    "##### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c38e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35bd22e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8710189713747241, 'start': 34, 'end': 42, 'answer': 'Playdata'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Squirrel and I work at Playdata in Seoul\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969303",
   "metadata": {},
   "source": [
    "- 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "355d35b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--klue--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_qna = pipeline('question-answering', model='klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3406dc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0002554136735852808,\n",
       " 'start': 155,\n",
       " 'end': 168,\n",
       " 'answer': '1933년부터는 각혈까지'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_qna(\n",
    "    question=\"1931년 어떤 진단을 받았나요?\",\n",
    "    context=\"\"\"그러던 1931년, 그는 갑작스럽게 폐결핵을 진단받았다. 이상은 하루에 담배를 50개피 이상 피는 것을 자신의 일과라고 표현했을 정도로 엄청난 골초였는데, 그 때문인지 병세는 날이 갈수록 악화되어 담당의가 그의 폐를 확인하고는 형체도 안 보인다며 혀를 내둘렀을 정도였다. 결국 1933년부터는 각혈까지 시작되었고, 건축기사 일을 지속하기 어렵다고 판단한 이상은 조선총독부에서 퇴사하고 황해도에 있는 배천 온천으로 요양을 간다.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31c19adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuestionAnsweringPipeline in module transformers.pipelines.question_answering object:\n",
      "\n",
      "class QuestionAnsweringPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  QuestionAnsweringPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |\n",
      " |  Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering\n",
      " |  examples](../task_summary#question-answering) for more information.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |\n",
      " |  >>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n",
      " |  >>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
      " |  {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n",
      " |  ```\n",
      " |\n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |\n",
      " |  This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"question-answering\"`.\n",
      " |\n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=question-answering).\n",
      " |\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |\n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      QuestionAnsweringPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Answer the question(s) given as inputs by using the context(s).\n",
      " |\n",
      " |      Args:\n",
      " |          question (`str` or `list[str]`):\n",
      " |              One or several question(s) (must be used in conjunction with the `context` argument).\n",
      " |          context (`str` or `list[str]`):\n",
      " |              One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
      " |              `question` argument).\n",
      " |          top_k (`int`, *optional*, defaults to 1):\n",
      " |              The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
      " |              top_k answers if there are not enough options available within the context.\n",
      " |          doc_stride (`int`, *optional*, defaults to 128):\n",
      " |              If the context is too long to fit with the question for the model, it will be split in several chunks\n",
      " |              with some overlap. This argument controls the size of that overlap.\n",
      " |          max_answer_len (`int`, *optional*, defaults to 15):\n",
      " |              The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
      " |          max_seq_len (`int`, *optional*, defaults to 384):\n",
      " |              The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\n",
      " |              model. The context will be split in several chunks (using `doc_stride` as overlap) if needed.\n",
      " |          max_question_len (`int`, *optional*, defaults to 64):\n",
      " |              The maximum length of the question after tokenization. It will be truncated if needed.\n",
      " |          handle_impossible_answer (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not we accept impossible as an answer.\n",
      " |          align_to_words (`bool`, *optional*, defaults to `True`):\n",
      " |              Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\n",
      " |              non-space-separated languages (like Japanese or Chinese)\n",
      " |\n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |\n",
      " |          - **score** (`float`) -- The probability associated to the answer.\n",
      " |          - **start** (`int`) -- The character start index of the answer (in the tokenized version of the input).\n",
      " |          - **end** (`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
      " |          - **answer** (`str`) -- The answer to the question.\n",
      " |\n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  get_answer(self, answers: list[dict], target: str) -> Optional[dict]\n",
      " |\n",
      " |  get_indices(self, enc: 'tokenizers.Encoding', s: int, e: int, sequence_index: int, align_to_words: bool) -> tuple[int, int]\n",
      " |\n",
      " |  postprocess(self, model_outputs, top_k=1, handle_impossible_answer=False, max_answer_len=15, align_to_words=True)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |\n",
      " |  preprocess(self, example, padding='do_not_pad', doc_stride=None, max_question_len=64, max_seq_len=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |\n",
      " |  span_to_answer(self, text: str, start: int, end: int) -> dict[str, typing.Union[str, int]]\n",
      " |      When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
      " |\n",
      " |      Args:\n",
      " |          text (`str`): The actual context to extract the answer from.\n",
      " |          start (`int`): The answer starting token index.\n",
      " |          end (`int`): The answer end token index.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dictionary like `{'answer': str, 'start': int, 'end': int}`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  create_sample(question: Union[str, list[str]], context: Union[str, list[str]]) -> Union[transformers.data.processors.squad.SquadExample, list[transformers.data.processors.squad.SquadExample]]\n",
      " |      QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the\n",
      " |      logic for converting question(s) and context(s) to [`SquadExample`].\n",
      " |\n",
      " |      We currently support extractive question answering.\n",
      " |\n",
      " |      Arguments:\n",
      " |          question (`str` or `list[str]`): The question(s) asked.\n",
      " |          context (`str` or `list[str]`): The context(s) in which we will look for the answer.\n",
      " |\n",
      " |      Returns:\n",
      " |          One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  default_input_names = 'question,context'\n",
      " |\n",
      " |  handle_impossible_answer = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |\n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          supported_models (`list[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |\n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |\n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |\n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |\n",
      " |      Return:\n",
      " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |\n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |\n",
      " |  get_inference_context(self)\n",
      " |\n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the 🤗 Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |\n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs: Any)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  dtype\n",
      " |      Dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ko_qna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
